---
title: Profiling and benchmarking
layout: default
---

```{r, echo = FALSE}
source("code/microbenchmark.R")
```

# Profiling and performance optimisation {#profiling}

> "We should forget about small efficiencies, say about 97% of the time:
> premature optimization is the root of all evil" --- Donald Knuth.

The key idea in this chapter can be summed up simply: "Find out what's then make it fast".  The first part of this chapter introduces you to tools to help understand what makes your code slow. The second part introduces you to some general tools for making your code faster.

Unfortunately optimisations are typically tightly coupled with the problem. It's hard to give give advice that will work in every situation, but I'll do my best. I include some general techniques, and so

Explore

Your code should be correct, maintainable and fast. Notice that speed comes last - if your function is incorrect or unmaintainable (i.e. will eventually become incorrect) it doesn't matter if it's fast. As computers get faster and R is optimised, your code will get faster all by itself. Your code is never going to automatically become correct or elegant if it is not already.

When making code faster be careful not to make it incorrect.


Like javascript, the vast majority of R code is poorly written and slow. This sounds bad but it's actually a positive! There's no point in optimising code until it's actually a bottleneck - most R code should be incredibly inefficient because even inefficient code is usually fast enough. If most R code was efficient, it would be a strong signal that R programmers are prematurely optimising, spend time making their code faster instead of solving real problems. Additionally, most people writing R code are not programmers. Many of them don't have any formal training in programming or computer science, but are using R because it helps them solve their data analysis problems.

This means that the vast majority of R code can be re-written in R to be more efficient. This often means vectorising code, or avoiding some of the most obvious traps discussed in the [R inferno] (http://www.burns-stat.com/documents/books/the-r-inferno/). There are also other strategies like caching/memoisation that trade space for time. Otherwise, a basic knowledge of data structures and algorithms can help come up with alternative strategies.

This applies not only to packages, but also to base R code. The focus on R code has been making a useful tool, not a blazingly fast programming language. There is huge room for improvement, and base R is only going to get faster over time.

That said, sometimes there are times where you need to make your code faster: spending several hours of your day might save days of computing time for others. The aim of this chapter is to give you the skills to figure out why your code is slow, what you can do to improve it, and ensure that you don't accidentally make it slow again in the future.  You may already be familiar with `system.time`, which tells you how long a block of code takes to run. This is a useful building block, but is a crude tool.

Along the way, you'll also learn about the most common causes of poor performance in R, and how to address them. Sometimes there's no way to improve performance within R, and you'll need to use C++, the topic of [Rcpp](#rcpp).

Having a good test suite is important when tuning the performance of your code: you don't want to make your code fast at the expense of making it incorrect. We won't discuss testing any further in this chapter, but we strongly recommend having a good set of test cases written before you begin optimisation.

Additionally, most people writing R code are not programmers. Many of don't have any formal training in programming or computer science, but are using R because it helps them solve their data analysis problems. This means that the vast majority of R code can be re-written in R to be more efficient. This often means vectorising code, or avoiding some of the most obvious traps discussed later in this chapter. There are also other strategies like caching/memoisation that trade space for time. Otherwise, a basic knowledge of data structures and algorithms can help come up with alternative strategies.

[Mature optimisation](http://carlos.bueno.org/optimization/mature-optimization.pdf) (PDF)

A recurring theme throughout this part of the book is the importance of differentiating between absolute and relative speed, and fast vs fast enough. First, whenever you compare the speed of two approaches to a problem, be very wary of just looking at a relative differences. One approach may be 10x faster than another, but if that difference is between 1ms and 10ms, it's unlikely to have any real impact. You also need to think about the costs of modifying your code. For example, if it takes you an hour to implement a change that makes you code 10x faster, saving 9 s each run, then you'll have to run at least 400 times before you'll see a net benefit.  At the end of the day, what you want is code that's fast enough to not be a bottleneck, not code that is fast in any absolute sense.  Be careful that you don't spend hours to save seconds.

Writing fast R code is part of the general task of becoming a better R programmer. As well as a the specific hints in this chapter, if you want to write fast R code, you'll need to generally improve your programming skills. Some ways to do this are to:

* [Read R blogs](http://www.r-bloggers.com/) to see what performance
  problems other people have struggled with, and how they have made their
  code faster.

* Read other R programming books, like
  [The Art of R Programming](http://amzn.com/1593273843).

* Take an algorithms and data structure course to learn some theory and
  well known ways of tackling certain classes of problems. I have heard
  good things about Princeton's
  [Algorithms](https://www.coursera.org/course/algs4partI) course offered by
  coursera.

You can also reach out to the community for help. Stackoverflow can be a useful place to ask, but you'll need to put some effort into creating an example that captures the salient features of your problem while being easily digestible. If it's too complex few people will have the time and motivation to attempt a solution. If it's too simple, you'll get answers that solve the toy problem, not the real problem. If you also try to answer questions on stackoverflow, you'll quickly get a feel for what makes a good question.


##### Prerequisites

In this chapter we'll be using the lineprof to understand performance of R code, so make sure you've installed it before continuing: `devtools::install_github("hadley/lineprof")`.

## Understanding performance

R provides a built in tool for profiling: `Rprof`. When active, this records the current call stack to disk every `interval` seconds. This provides a fine grained report showing how long each function takes. The function `summaryRprof` provides a way to turn this list of call stacks into useful information. But I don't think it's terribly useful, because it makes it hard to see the entire structure of the program at once. Instead, we'll use the `profr` package, which turns the call stack into a data.frame that is easier to manipulate and visualise.

Example showing how to use profr.

Sample pictures.

Things you can't profile with the line profiler:

* C/C++ code
* "special" primitives
* byte code compiled code

Other tools:
* https://github.com/ltierney/Rpkg-proftools - show flamegraph and call graph

## Improving performance

Once you've identified one specific bottleneck, you need to make that bottleneck faster. There are four techniques that I've found apply to many performance problems:

1. Look for existing solutions
1. Do less work
1. Vectorise
1. Avoid copies

It's also often useful to rewrite in a faster language like C++. This is a big topic so is subject of a complete chapter, [Rcpp](#rcpp). Two other techniques are slightly less important, but still useful to know about:

1. Byte-code compile
1. Trade space for time

Don't forget that clarity and readibility are more important than speed. Your intution for bottlenecks is likely to be bad, so don't sacrifice readability for performance unless you _know_ it will have a significant impact on run-time.

Before we get into the specific techniques, first I'll describe a general strategy and organisation style when working on performance.

### Code organisation

There are two traps that it's easy to fall into when making your code faster:

1. Making it faster by making it incorrect
1. Thinking you've made it faster, but you've actually make it slower.

You can avoid both these pitfalls with a good strategy. I recommend organising as follows.

Start by writing a function for each variation. The function should encapsulate all relevant behaviour, making it easy to test and time. Record everything you try, even if it ends up being slower: if you come back to problem in the future, it's useful to see both successful and unsuccessful strategies. I often work in a Rmarkdown file so that I can more easily intermingle code and explanation.

Start with your original function, and include any other variations that you've brainstormed.

```{r}
mean1 <- function(x) mean(x)
mean2 <- function(x) sum(x) / length(x)
```

Next, generate a representative test case. This should ideally take a couple of seconds to run, while capturing the essence of your problem. You are going to run this very frequently during testing so you don't want it to take too long, but you want it be as similar as possible to your real data. You may need to generate more than one test case if you're intersted in behaviour for different sized inputs.

```{r}
x <- runif(100)
```

Use this test case to make sure that all variants return the same result with `stopifnot()` and `all.equal()`. If your problem has fewer possible outputs, you may need multiple tests.

```{r}
stopifnot(all.equal(mean1(x), mean2(x)))
```

Finally, use the microbenchmark package to precisely compare how long each variation takes to run. For bigger problems, you may need to reduce the `times` paramater so that it only takes a couple of seconds to run the benchmark.

```{r}
microbenchmark(
  mean1(x),
  mean2(x)
)
```

You should also note your target speed: how fast does the algorithm need to perform so that it's no longer the bottleneck. It's important to determine this so that you don't waste time optimising code that is no longer a bottleneck.

If you'd like to see this strategy in action, here are some stackoverflow questions that use it:

* http://stackoverflow.com/questions/22515525#22518603
* http://stackoverflow.com/questions/22515175#22515856
* http://stackoverflow.com/questions/3476015#22511936

### Has someone already solved the problem?

Once you've organised your code, capturing the variations you've thought up, it's natual to see what others have down. One of the easiest ways to speed up your code is to find existing faster code. You are part of a large community, and it's quite possible that someone else has had the same problem as you. If your bottleneck is in another package, two good places to start looking at:

* [CRAN task views](http://cran.rstudio.com/web/views/). If there is a
  CRAN task view related to your problem domain, it's worth looking for
  alternative packages.

* Reverse dependencies of Rcpp, as listed on its
  [CRAN page](http://cran.r-project.org/web/packages/Rcpp). Since these
  packages use C++, it's possible they have implemented your bottleneck
  in a higher performance language.

Otherwise the challenge is describing your bottleneck in words in a way that helps you search for related problems and solutions. You will be aided if you know what the name of the problem is and some common synonyms. It's hard to search for this knowledge (because you don't know what it's called!) but you can build it up over time by reading broadly about statistics and algorithms, or you can ask others. Once you've brainstormed some possible names, search on google and stackoverflow. Make sure you know good ways of restricting your search to R. For google, try [rseek](http://www.rseek.org/); for stackoverflow, restrict your searches to the R tag with `[R]`.

Include each solution you find in your file. You want to record all solutions, not just those that are already faster. Some solutions might slower to start with, but easier to optimise using the techniques below, and so end up faster than your original technique. You may also be able to combine the fastest parts of different approaches.

### Exercises

1.  What faster alternatives to `lm` are available?

1.  What package implements a version of `match()` that's faster for
    repeated look ups? How much faster is it?

1.  List four functions (not just in base R) that convert a string into a
    date time? What are their strengths and weaknesses?

1.  How many different ways can you compute a 1d density estimate in R?

1.  What packages provide the ability to compute a rolling mean?

1.  What alternatives to `optim()` are available?

### Do as little as possible

The second easiest way to make code faster is to make it do less work. Sometimes you can use a faster, more specific function:

* `vapply()` is faster than `sapply()`, because you pre-specified the output
  type.

* `rowSums()`, `colSums()`, `rowMeans()`, and `colMeans()` are faster than
  the equivalent `apply()` invocations because they are vectorised (the topic
  of the next section).

* If you want to see if a vector contains a single value, `any(x == 10)`
  is much faster than `10 %in% x` because testing equality is simpler than
  testing for inclusion in a set.

Having these specific functiosn at your fingertips is a matter of having a good R [vocabulary](#vocabulary). The best way to expand your vocabulary over time is to regularly regularly read R code, like on R-help or on [stackoverflow](stackoverflow.com/questions/tagged/r).

Other functions will do less work if you give them more information about the problem. It's always worthwhile to carefully read the documentation and experiment with different arguments. Some examples that I've discovered in the past:

* `read.csv()`: specify known the columns types with `colClasses`

* `factor()`: specify known levels with `levels`

* `cut()`: don't generate labels with `labels = FALSE` if you don't need them
  (or even better use `findInterval()` as mentioned in the see also section of
  the documentation.)

* `interaction()`: if you only need combinations that exist in the data, use
  `drop = TRUE`

Sometimes you can make a function faster by avoiding method dispatch. As we've seen ([Extreme dynamism](#extreme-dynamism)) method dispatch in R can be costly, so if you're calling a method in a tight loop, you can avoid some of the cost by doing method lookup once. For S3, you can do this manually by calling `generic.class()` instead of `generic()`. For S4, you can use `findMethod()` to find the method, then save it in a local variable. For example, calling `mean.default()` instead of `mean()` is quite a bit faster for small vectors:

```{r}
x <- runif(1e2)

microbenchmark(
  mean(x),
  mean.default(x)
)
```

`mean()` is safe, but slow; `mean.default()` is almost twice as faster, but will fail in surprising ways if `x` is not a vector.

Other times, if you know you're dealing with a specific type of input, you can a faster strategy. For example, `as.data.frame()` is quite slow because it first coerces each element to a data frame and then `rbind()`s them together. If have a named list with vectors of equal lengths, you can turn it into a data frame directly. With the strong assumption that we have a list that's ok as is, we can generate a method that's about 20x faster than the default.

```{r}
quickdf <- function(l) {
  class(l) <- "data.frame"
  attr(l, "row.names") <- .set_row_names(length(l[[1]]))
  l
}

l <- lapply(1:26, function(i) runif(1e3))
names(l) <- letters

microbenchmark(
  quickdf(l),
  as.data.frame.list(l),
  as.data.frame(l)
)
```

Note the common tradeoff: this method is fast because it's dangerous. If you give bad inputs you'll get a corrupt data frame.

```{r}
quickdf(list(x = 1, y = 1:2))
```

To come up with this minimal method, I carefully read through then rewrote the source code for `as.data.frame.list()` and `data.frame()`. I made many small changes, each time checking that I hadn't broken existing behaviour. After several hours work, I'd isolated the minimal code shown above.

This is a very useful technique. Most base R functions are written for flexiblity and functionality, not performance, and often rewriting for your specific need can yield substantial speed ups. To do this, you'll need to read the source code. It will often be complex and confusing, but don't give up!

The following example shows a progressive simplification of the `diff()` function for the special case of computing differences between adjacent values in a vector. At each step, I replace one arguments with a specific case, then check that the function still works. The initial function is long and complicated, but by restricting the arguments I not only make it around twice as fast, I also make it easier to understand.

```{r}
# The original function, reformatted after typing diff
diff1 <- function (x, lag = 1L, differences = 1L) {
  ismat <- is.matrix(x)
  xlen <- if (ismat) dim(x)[1L] else length(x)
  if (length(lag) > 1L || length(differences) > 1L || lag < 1L || differences < 1L)
    stop("'lag' and 'differences' must be integers >= 1")

  if (lag * differences >= xlen) {
    return(x[0L])
  }

  r <- unclass(x)
  i1 <- -seq_len(lag)
  if (ismat) {
    for (i in seq_len(differences)) {
      r <- r[i1, , drop = FALSE] - r[-nrow(r):-(nrow(r) - lag + 1L), ,
        drop = FALSE]
    }
  } else {
    for (i in seq_len(differences)) {
      r <- r[i1] - r[-length(r):-(length(r) - lag + 1L)]
    }
  }
  class(r) <- oldClass(x)
  r
}

# Step 1: Assume vector input. This allows me to remove the is.matrix()
# test and the method that use matrix subsetting.
diff2 <- function (x, lag = 1L, differences = 1L) {
  xlen <- length(x)
  if (length(lag) > 1L || length(differences) > 1L || lag < 1L || differences < 1L)
    stop("'lag' and 'differences' must be integers >= 1")

  if (lag * differences >= xlen) {
    return(x[0L])
  }

  i1 <- -seq_len(lag)
  for (i in seq_len(differences)) {
    x <- x[i1] - x[-length(x):-(length(x) - lag + 1L)]
  }
  x
}
diff2(cumsum(0:10))

# Step 2: assume difference = 1L. This simplifies input checking
# and eliminates the for loop
diff3 <- function (x, lag = 1L) {
  xlen <- length(x)
  if (length(lag) > 1L || lag < 1L)
    stop("'lag' must be integer >= 1")

  if (lag >= xlen) {
    return(x[0L])
  }

  i1 <- -seq_len(lag)
  x[i1] - x[-length(x):-(length(x) - lag + 1L)]
}
diff3(cumsum(0:10))

# Step 3: assume lag = 1L. This eliminates input checking and simplifies
# subsetting.
diff4 <- function (x) {
  xlen <- length(x)
  if (xlen <= 1) return(x[0L])

  x[-1] - x[-xlen]
}
diff4(cumsum(0:10))

x <- runif(100)
microbenchmark(
  diff1(x),
  diff2(x),
  diff3(x),
  diff4(x)
)
```

Once you've read [Rcpp](#rcpp) you'll be able to make `diff()` considerably faster for this special case.

A final example of doing less work is to work with a simpler data structure. For example, if you subsetting rows from a data frame, instead of working with data frames it's often much faster to work with indices. For example, if you wanted to compute a bootstrap estimate of the mean, there are two basic approaches:

1. Resample the entire data frame
1. Resample column indices

These two strategies are implemented below:

```{r}
boot_indices <- function(df, i) sample.int(nrow(df), i, replace = TRUE)
boot_indices(mtcars, 10)

boot_rows <- function(df, i) {
  df[boot_indices(df, i), , drop = FALSE]
}

microbenchmark(
  indices = mean(mtcars$cyl[boot_indices(mtcars, 10)]),
  rows = mean(boot_rows(mtcars, 10)$cyl)
)
```

Working with indices instead of with data frames is more complicated, but is about 8 times faster. Data frames are one of the slowest data structures in R, so if you can avoid using them it will often make your code faster, at the cost of making it harder to understand.

### Exercises

1.  How do the results change if you compare `mean()` and `mean.default()`
    on 10,000 observations, rather than on 100?

1.  Make a faster version of `chisq.test()` that only computes the Chi-square
    test statistic when the input is two numeric vectors with no missing
    values. You can either start from `chisq.test()` and make it simpler,
    or trying starting from
    [the definition](http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test).

1.  Can you make a faster version of `table()` for the special case of
    two integer input vectors with no missing values? Can you use it to
    speed up your Chi-square test?

### Vectorisation

Vectorisation is a powerful tool. Doesn't mean using `apply()` or `lapply()` or even `Vectorise()`. Those just change the interface of the function without changing the performance. A vectorised function takes vectors as inputs and does the loop in C, avoiding the overhead of R function calls and explicitly modifying objects in place. You'll learn how to write your own vectorised functions in [Rcpp](#rcpp). But taking advantage of already vectorised functions in R is also really important.

* `rowSums()`, `colSums()`, `rowMeans()`, and `colMeans()` are vectorised
  matrix functions are will always be faster than `apply()`.

* Be aware of the most efficient ways to convert continuous to categorical
  values (`findInterval()`) and to re-name categorical values (character
  subsetting).

* Be aware of vectorised functions like `cumsum()` and `diff()`.

* Whole object subsetting. `x[is.na(x)] <- 0` will replace all missing
  values in `x` with 0 if `x` is a vector, matrix or data frame.

Not about eliminating for-loops, about thinking about the problem in a whole-object, vectorised way.

A special case of vectorisation is matrix algebra, where the loops are done by highly tuned external libraries like BLAS. If you can figure out a way to use matrix algebra to solve a problem, it will often be very fast.

* `tcrossprod()`.

* Weighted sums: instead of doing `sum(weight * x)`, do `crossprod(weight, w)`

* Dividing by row/column means

But it's not always possible to apply it directly, and you may need to understand the underlying method. The following case study explores how to make many t-tests faster, following "Computing thousands of test statistics simultaneously in R" by Holger Schwender and Tina Müller in http://stat-computing.org/newsletter/issues/scgn-18-1.pdf - read the paper to see this method applied to other tests.

### Case study: t-test

We can combine vectorisation and doing the minimum to make the t-test substantially faster. Imagine we have run 1000 experiments (rows), each of which collected data on 50 individuals (cols). The first 25 individuals in each experiement were assigned to group 1 and the others to group 2.

```{r}
m <- 1000
n <- 50
X <- matrix(rnorm(m * n, 10, 3), m)
grp <- rep(1:2, e = n / 2)
```

There are two basic ways to use `t.test()` with a formula or with two vectors, once for each group. Timing these reveals that the formula interface is considerably slower.

```{r, cache = TRUE}
system.time(for(i in 1:m) t.test(X[i, ] ~ grp)$stat)
system.time(for(i in 1:m) t.test(X[i, grp == 1], X[i, grp == 2])$stat)
```

Of course, a for loop just computes, but doesn't save, so we might want to use `apply()` instead. This adds little overhead:

```{r}
compT <- function(x, grp){
  t.test(x[grp == 1], x[grp == 2])$stat
}
system.time(apply(X, 1, compT, grp = grp))
```

How can we make this faster? First, we could try doing less work. If you look at the source code, `stats:::t.test.default()`, you can see it does a lot more than just computing the t-statistic. It also computes the p-value and creates nice output for printing. Maybe we can make our code faster by stripping out those pieces.

```{r}
my_t <- function(x, grp) {
  t_stat <- function(x) {
    m <- mean(x)
    length <- length(x)
    var <- sum((x - m) ^ 2) / (n - 1)

    list(m = m, n = n, var = var)
  }

  g1 <- t_stat(x[grp == 1])
  g2 <- t_stat(x[grp == 2])

  pooled_se <- sqrt(g1$var / g1$n + g2$var / g2$n)
  (g1$m - g2$m) / pooled_se
}
system.time(apply(X, 1, my_t, grp = grp))
```

That gives us about a 5x speed up.

Now that we have a fairly simple function, we can make it faster still by vectorising it. Instead of looping over the array outside the funtion, we vectorise the function, modifying `t_stat()` to work with a matrix of values instead of a vector: `mean()` becomes `rowMeans()`, `length()` becomes `ncol()`, and `sum()` becomes `rowSums()`. The rest of the code stays the same.

```{r}
rowtstat <- function(X, grp){
  t_stat <- function(X) {
    m <- rowMeans(X)
    n <- ncol(X)
    var <- rowSums((X - m) ^ 2) / (n - 1)

    list(m = m, n = n, var = var)
  }

  g1 <- t_stat(X[, grp == 1])
  g2 <- t_stat(X[, grp == 2])

  pooled_se <- sqrt(g1$var / g1$n + g2$var / g2$n)
  (g1$m - g2$m) / pooled_se
}
system.time(rowtstat(X, grp))
```

That's much faster! At least 40x faster than our previous best effort, and around 1000x faster than where we started.

### Avoid copies

One of the most pernicious causes of slow code is inadvertently modifying an object in a loop in such a way that every modification requires the complete object to be copied. Sometimes this happens because R isn't always very good at picking up in place modifications, for example, if you modified a single element in a data frame the entire data frame is copied. Other times, it's because you haven't thought through the implications:

* every time you add a new element to a vector with `c()` or `append()` the
  entire vector must be copied

* every time you add on to an existing matrix with `cbind()` or `rbind()`
  the entire matrix must be copied

* every time you make a longer string with `paste()` the complete string
  must be copied.

Here's a little benchmark that illustrates the difference. We first generate some random strings, and then combine them either iteratively with a loop with `collapse()`, or once with `paste()`. Note that the peformance of `collapse()` get relatively worse as the number of strings grows: combining 100 strings takes almost 30 times longer than combining 10 strings.

```{r}
random_string <- function() {
  paste(sample(letters, 50, replace = TRUE), collapse = "")
}
strings10 <- replicate(10, random_string())
strings100 <- replicate(100, random_string())

collapse <- function(xs) {
  out <- ""
  for (x in xs) {
    out <- paste0(out, x)
  }
  out
}

microbenchmark(
  collapse(strings10),
  collapse(strings100),
  paste(strings10, collapse = ""),
  paste(strings100, collapse = "")
)
```

This is Circle 2 in the [R inferno](http://www.burns-stat.com/pages/Tutor/R_inferno.pdf). More examples at https://gist.github.com/wch/7107695.

[Modification in place]{#modification-in-place} shows some other more subtle examples of this phenomena, and gives you tools to determine whether an object is indeed being modified in place, or is being copied multiple times.

### Byte code compilation

R 2.13.0 introduced a new byte code compiler which can increase the speed of certain types of code 4-5 fold. This improvement is likely to get better in the future as the compiler implements more optimisations - this is an active area of research.

Using the compiler is an easy way to get speed ups - it's easy to use, and if it doesn't work well for your function, then you haven't invested a lot of time in it, and so you haven't lost much. The following example shows the pure R version of `lapply()` from [functionals](#lapply). Compiling it gives a considerable speedup, although it's still not quite as fast as the C version provided by base R.

```{r}
lapply2 <- function(x, f, ...) {
  out <- vector("list", length(x))
  for (i in seq_along(x)) {
    out[[i]] <- f(x[[i]], ...)
  }
  out
}

lapply2_c <- compiler::cmpfun(lapply2)

x <- list(1:10, letters, c(F, T), NULL)
microbenchmark(
  lapply2(x, is.null),
  lapply2_c(x, is.null),
  lapply(x, is.null)
)
```

This is a relatively good example for byte code compiling. In most cases you're more like to get a 5-10% speedup. This example optimises well because it uses a for-loop, something that is generally rare in R.

All base R functions are byte code compiled by default.

### Trade space for time

A very general optimisation technique is to trade space for time by caching results. Instead of computing repeatedly, you compute once and then look up again in the future. A special case of caching is memoisation.

http://en.wikipedia.org/wiki/Dynamic_programming: "The idea behind dynamic programming is quite simple. In general, to solve a given problem, we need to solve different parts of the problem (subproblems), then combine the solutions of the subproblems to reach an overall solution. Often when using a more naive method, many of the subproblems are generated and solved many times. The dynamic programming approach seeks to solve each subproblem only once, thus reducing the number of computations: once the solution to a given subproblem has been computed, it is stored or "memo-ized": the next time the same solution is needed, it is simply looked up. This approach is especially useful when the number of repeating subproblems grows exponentially as a function of the size of the input."

In R, you should be most familiar with `readRDS()` and `saveRDS()`. They allow you to save and load individual R object to disk. Use them in preference to `save()` and `load()` which work with complete workspaces. Depending on the speed of your disk, how much disk space you have free and the speed of your CPU, it may be worth setting `compress = FALSE`. You can make richer trade-offs between disk space and compression time by doing the compression yourself: see the `?saveRDS` examples for details.

Caching packages: memoise, hash, http://cran.r-project.org/web/packages/cacher/index.html, http://cran.r-project.org/web/packages/R.cache/index.html

### Exercises

1.  Imagine you want to compute the boostrap distribution of a sample
    correlation. In other words you have data like in the example below, and
    you want to run `cor_df()` many times. How can you make this code faster?
    (Hint: the function does three things that you can speed up.)

    ```{r, eval = FALSE}
    n <- 1e6
    df <- data.frame(a = rnorm(n), b = rnorm(n))

    cor_df <- function(i) {
      i <- sample(seq(n), n * 0.01)
      cor(q[i, , drop = FALSE])[2,1]
    }
    ```

    Is there a way to vectorise this procedure?
