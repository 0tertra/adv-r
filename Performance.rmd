---
title: Performance
layout: default
---

# Performance

```{r, echo = FALSE}
library(microbenchmark)
options(digits = 3)
options("microbenchmark.unit" = "ns")
```

<!--
Once more complete, circulate to Justin, Radford, Alex & Jan for comments.
-->

## Introduction

As far as computer languages go, R is pretty slow. However, in practice this isn't usually a problem. In data analysis, the bottleneck is usually your brain: you spend more time thinking about what you should compute compared to actually performing the computations. However, as you start to develop more complicated code that works on large datasets, you may find computation becomes a bottleneck. That's because R's design is not optimised for performance, but instead it's optimised for expressiveness.  John Chambers, the designer of the S language that underlies R, recognised that the biggest bottleneck in most data analyses is congitive, not computational. It takes much longer to figure out what you want do, and express it 

For most data analysis problems, the bottleneck is cognitive, not computational: you spend more time thinking about what you should be doing than having the computer actually do it. The R language is optimised for expressiveness, not speed, which is the right tradeoff for most problems. The flexibility of the R language makes it harder to optimise, but it makes it easy to build domain specific languages that allow you to clearly express what you want.

You didn't start using R because it was a fast language, you probably started using it because it helped you solve a problem.

The goal of this part of the book is to give you a deeper understanding into R's performance characteristics. You'll learn where R is fast, where it's slow, and how you can improve speed when you need to.

* In this chapter, I'll discuss some of the reasons why R is slow, and help you build a better intuition for what operations are likely to be problematic. I'll also talk a little about the future of R, and what performance problems may improve in the future. Throughout the chapter I'll use microbenchmarking as a tool to quantitatively explore R's performance characterisics. 

* In [Profiling](#profiling), you'll learn concrete tools for making your code faster by first figuring out what you need to optimise and then learning some general tools to optimise it. 

* In [Memory](#memory), you'll learn about how R works with memory, and some common performance problems.

* For really high-performance code, you often need to use another programming language. [Rcpp](#rcpp) will teach you the absolute minimum you need to know about C++ in order to write fast C++ code linked to R through Rcpp.

* Finally, if you want to deeply understand the performance of built in base functions, you'll need to learn a little bit about R's C api. In [R's C interface](#c-api), you'll learn more about the R's C internals and see how some common built-in functions work.

A recurring theme throughout this part of the book is the importance of differentiating between absoluate and relative speed, and fast vs fast enough. First, whenever you compare the speed of two approaches to a problem, be very wary of just looking at a relative differences. One approach may be 10x faster than another, but if that difference is between 1ms and 10ms, it's unlikely to have any real impact. You also need to think about the costs of modifying your code. For example, if it takes you an hour to implement a change that makes you code 10x faster, saving 9 s each run, then you'll have to run at least 400 times before you'll see a net benefit.  At the end of the day, what you want is code that's fast enough to not be a bottleneck, not code that is fast in any absolute sense.  Be careful that you don't spend hours to save seconds.

1 ms means 1000 calls take one second, 1 µs means needs 1,000,000 calls, 1 ns needs 1,000,000,000 calls. 

## Why is R slow?

By and large R is slow because it is optimised for expressiveness, not speed. This is a tradeoff between writing and running code: R makes it faster for you to write code at the cost of making it harder for the computer to run it. Different languages occupy different positions in the space of expressiveness vs performance, which is one of the reasons why one language isn't appropriate for every task.


The downside of mingling two languages is that you need more knowledge (i.e. now you need to know some C++ as well as R) to understand code. In practice, this is not a big drawback because:

1. Often the people writing the high-performance code and the people using it are different. For every package author who has to learn a little C++, there may be thousands (or tens of thousands) of people who use the package and only need to understand R.

2. There are extremely few environments where you can get away with only knowing one language. Is your data in a database? You'll need to know some SQL. Are you working with strings? You'll need to know regular expressions. Are you doing a lot of web scraping? You'll probably need to know some ruby or python. Does your company have a big hadoop cluster? You'll probably need to know some java.

While R suffers from the theoretical inelegance of not being "turtles all the way" down, this is not a big draw back in practice. Most data scientists will be expert in one language, and have basic skills in a handful of others.

At the end of this chapter you might feel like R is so impossibly slow that you can never achieve anything in it, or when you try and tackle some problems you'll feel parallelised because you know what ever approach you take will be hopelessly inefficient. Just remember that while R might be slow compared to other programming languages, for most tasks is fast enough. All of the timings in the microbenchmarks are in nanoseconds and unless you are calling functions billions of times, these inefficiencies don't matter. R has been carefully designed to save time for data analysts (whose time is valuable), not to save time for computers (whose time is cheap). Also just because R slow as a language does not mean it will be slow for a specific task. Many common data analysis bottlenecks have been rewritten in C or Fortran for better speed.

The fundamental reason that R is slow is that it was designed to be expressive, not fast. There's a tradeoff between expressiveness, speed and ease of implementation. It's easy to make a language that's fast but limited, or expressive but slow. It's hard to make a language that's both expressive and performant. That's not to say that R can't be made much faster. A ten-fold improvement in speed seems achievable, and in [faster R](#faster-r) I'll discuss some of the promising new implementations of R that show how R might do better in the future.



## Microbenchmarking

A microbenchmark is a performance measurement of a very small piece of code, something that might take microseconds (µs) or nanoseconds (ns) to run. In this chapter, I'll use microbenchmarks to demonstrate the performance of very low-level pieces of R, to help develop your intution for how R works. This intuition, by-and-large, is not practically useful because in most real slow code saving microseconds does not have an appreciable effect on run time. You should not change the way you code because of these microbenchmarks, but instead wait until the next chapter to learn how to speed up real code.

The best tool for microbenchmarking in R is (surprise!) the [microbenchmark][microbenchmark] package, which offers very precise times. This makes it much easier to compare operations that only take a small amount of time. The following code shows a simple example of using `microbenchmark()` to compare two ways of computing a square root:

```{r}
library(microbenchmark)

x <- runif(100)
microbenchmark(
  sqrt(x),
  x ^ 0.5
)
```

Not surprisingly, using a special purpose function is faster than a general purpose function.

My favourite features of the microbenchmark package are:

* The timing of every input expression is repeated 100 times, interleaved in random order. This allows you to see the variability associated with the estimates, while controlling for systematic variability.

* All individual timings are stored in its output, but only summaries are displayed with the default method of `print()`.

* You can see the variability in timings with `boxplot()`, or if if ggplot2 is loaded, `autoplot()` methods.

### Exercises

* Compare the results `microbenchmark()` from `system.time()` (you'll need to use a loop so that they're long enough to register with `system.time()`). Why are the estimates of the individual operations different?

    ```{r, eval = FALSE}
    n <- 1:1e6
    system.time(for (i in n) sqrt(x)) / length(n)
    system.time(for (i in n) x ^ 0.5) / length(n)
    ```

* Here are a few other ways to compute the square root of a vector. Which do you think will be fastest? Which will be slowest? Use microbenchmarking to confirm your answers.

    ```{r, eval = FALSE}
    x ^ (1 / 2)
    exp(log(x) / 2)
    ```

## Fundamental reasons

In this section I'll explore three fundamental reasons why R is slow: extreme dynamism, few built-in constants, and lazy evaluation of function arguments. I'll illustrate each reason with a microbenchmark, showing how it slows R down. Each reason only causes a tiny slowdown, on the order of a few microseconds per function call, but together they add up. When looking at millions or billions of function calls, which happens when looping over long vectors or doing big simulations, they make R a relatively slow language. 

These reasons fundamentally limit the maximum performance of the R language. They are all illustrates features where R has traded speed for expressiveness. 

If you'd like to learn more about the performance characteristics of R, I'd particularly recommend the article [Evaluating the Design of the R Language](https://www.cs.purdue.edu/homes/jv/pubs/ecoop12.pdf) by Floreal Morandat, Brandon Hill, Leo Osvald and Jan Vitek. It discusses a better methodology for understand the performance characteristics of R using a modified R interpreter and a wide set of real code.

### Extreme dynamism

R is an extremely dynamic programming language, and almost anything can be modified after it's created. You can:

* change the body, arguments and environment of functions
* change the S4 methods for a generic
* add new fields to an S3 object, or even change its class
* modify objects outside of the local environment with `<<-`

Dynamism makes code slow because it makes it difficult to predict exactly what will happen in a given function call. The easier it is to predict what's going to happen, the more likely an interpreter or compiler can jump directly to the fastest implementation. (If you'd like more details, Charles Nutter expands on this idea at [On Languages, VMs, Optimization, and the Way of the World](http://blog.headius.com/2013/05/on-languages-vms-optimization-and-way.html).) If a compiler can't predict what's going to happen, it has to look through many options to find the best one, which is an expensive operation.

The following microbenchmark illustrates the cost of method dispatch for S3, S4, and RC. I create a generic and a method for each OO system, then call the generic and see how long it takes to find and call the method. I also time how long it takes to call the bare method for comparison.

```{r, results = 'hide'}
f <- function(x) NULL

s3 <- function(x) UseMethod("s3")
s3.integer <- f

A <- setClass("A", representation(a = "list"))
setGeneric("s4", function(x) standardGeneric("s4"))
setMethod(s4, "A", f)

B <- setRefClass("B", methods = list(rc = f))

a <- A()
b <- B$new()
```

```{r}
microbenchmark(
  fun = f(),
  S3 = s3(1L),
  S4 = s4(a),
  RC = b$rc()
)
```

On my computer, the function call takes about 280 ns. S3 method dispatch takes an additional 3,000 ns; S4 dispatch, 13,000 ns; and RC dispatch, 11,000 ns. Method dispatch is so expensive because R must look for the function every time the method is called; it might have changed between this time and the last time. R could do better by caching methods between calls, but caching is hard to do correctly and a notorious source of bugs. In many situations, compiled languages only need to method lookup once, during compilation, because they know that method can't be change at run-time. This takes the cost of method dispatch down to effectively 0.

### Few built-in constants

Another example of R's dynamisms is the very small number of built-in constants.
Almost every operation is a lexically scoped function call. For example, in the following simple function contains four function calls: `{`, `(`, `+`, `^`.

```{r}
f <- function(x, y) {
  (x + y) ^ 2
}
```

These functions are not hard coded constants, and can be overriden by you. This means that to find the definition of each function, R has to look through every environment on the search path, which could easily be 10 or 20 environments. It would be possible to change this behaviour for R, and affect very little code (it's a bad idea to override `{` or `(`!), but it would require substantial work by R core.

The following microbenchmark hints at the performance costs. We create four versions of `f()`, each with one more environment (containing 26 bindings) between the environment of `f()` and the base environment where `+`, `^`, `(`, and `{` are defined. 

```{r}
random_env <- function(parent = globalenv()) {
  letter_list <- setNames(as.list(runif(26)), LETTERS)
  list2env(letter_list, envir = new.env(parent = parent))
}
set_env <- function(f, e) {
  environment(f) <- e
  f
}
f2 <- set_env(f, random_env())
f3 <- set_env(f, random_env(environment(f2)))
f4 <- set_env(f, random_env(environment(f3)))

f_b <- set_env(f, baseenv())

microbenchmark(
  f(1, 2),
  f2(1, 2),
  f3(1, 2),
  f4(1, 2), 
  times = 1000
)
```

Each additional environment between `f()` and the base environment makes the function slower by about 50ns.  Most other languages have many more built in constants that you can't override. This means that they always know exactly what `+`, `-`, `{` and `(` and they don't need to waste time repeatedly looking up their definitions. The cost of that decision is it means make writing the interpreter a little harder (because there are more special cases), and the language isn't quite as flexible.

### Lazy evaluation overhead

In R, functions arguments are evaluated lazily (as discussed in [lazy evaluation](#lazy-evaluation) and [capturing expressions](#capturing-expressions)). To implement lazy evaluation, when a function is called, R creates a promise object that contains the expression needed to compute the result, and the environment in which to perform the computation. Creating these objects has some overhead, so every additional argument to an R function slows it down a little.

The following microbenchmark compares the run time of a very simple function. Each version of the function has one extra argument, which allows us to see that each an additional argument costs about 20 ns.

```{r promise-cost}
f0 <- function() NULL
f1 <- function(a = 1) NULL
f2 <- function(a = 1, b = 1) NULL
f3 <- function(a = 1, b = 2, c = 3) NULL
f4 <- function(a = 1, b = 2, c = 4, d = 4) NULL
f5 <- function(a = 1, b = 2, c = 4, d = 4, e = 5) NULL
m <- microbenchmark(f0(), f1(), f2(), f3(), f4(), f5(), times = 1000)
```

In most other programming languages there is no overhead for adding extra arguments. Many compiled languages will even warn you if arguments are never used, and automatically remove them from the function.

## Incidental reasons

R's design limits its maximum theoretical performance. But R is currently nowhere that maximum, and there are many things that can (and will) be done to speed it up. This section discusses some of the parts of R that are currently not for fundamental reasons, but because no one has had the time to make them fast.

R is over 20 years old, and contains nearly 800,000 lines of code. All changes to base R are made by R Core Team (or R-core for short). R-core contains [20 members](http://www.r-project.org/contributors.html), but only about six are actively involved in day-to-day development. No one on R-core is paid to work on R full time. Most members of R-core are statistics professors, and can only spend a relatively small amount of their time working on R. 

Because R-core lacks full-time software developers, it has accumulated considerable technical debt over time. One component of this debt is the lack of unit tests. Without unit tests, it's difficult to improve performance without accidentally changing behaviour. Because of the care that must be taken to avoid breaking existing code, R-core tends to be very conservation about accepting new code. It can be frustrating to see obvious performance improvements rejected by R-core, but the driving motivation for R-core is not to make R faster, it's to build a stable platform for statistical computing.

Next, I'll show three parts of R that are currently slow, but could be made faster with some effort. They are not critical parts of base R, but they have frustrated me in the past. As with the microbenchmarks above, these do not generally impact the average performance of most funtion, but can be important for special cases.

The following microbenchmark shows seven ways to access a single value (the number in the bottom-right corner) from the built-in `mtcars` dataset. The variation in performance is startling: the slowest method takes 30x longer than the method. There's no reason for there to be such a huge difference in performance, but no one has spent the time to make the slowest methods faster.

```{r}
microbenchmark(
  mtcars["Volvo 142E", "carb"],
  mtcars[32, 11],
  mtcars[[c(11, 32)]],
  mtcars[["carb"]][32],
  mtcars[[11]][32],
  mtcars$carb[32],
  .subset2(mtcars, 11)[32],
  unit = "us"
)
```

Some base functions are known to be slow. For example, look at the following three implementations of a function to `squish()` a vector to make the sure smallest value is at least `a` and the largest value is at most `b`.  The first implementation, `squish_ife()` uses `ifelse()`. `ifelse()` is known to be slow because it is relatively general, and must evaluate all arguments fully. The second implementation, `squish_p()`, uses `pmin()` and `pmax()` which should be much faster because they're so specialised. But they're actually rather slow because they can take any number of arguments and have to do some relatively complicated checks to determine which method to use. The final implementation uses basic subassignment.

```{r}
squish_ife <- function(x, a, b) {
  ifelse(x <= a, a, ifelse(x >= b, b, x))
}
squish_p <- function(x, a, b) {
  pmax(pmin(x, b), a)
}
squish_in_place <- function(x, a, b) {
  x[x <= a] <- a
  x[x >= b] <- b
  x
}

x <- runif(100, -1.5, 1.5)
microbenchmark(
  squish_ife(x, -1, 1),
  squish_p(x, -1, 1),
  squish_in_place(x, -1, 1),
  unit = "us"
)
```

There's quite a variation in speed: using `pmin()` and `pmax()` is about 3x faster than using `ifelse()`, and using subsetting directly is about twice as fast again. As we'll see in [Rcpp](C++), we can often do even better by calling out to C++. The following example compares the best R implementation to a relatively simple (if verbose) implementation in C++. The C++ implementation is around 3x faster.

```{r}
library(Rcpp)
cppFunction("NumericVector squish_cpp(NumericVector x, double a, double b) {
  int n = x.length();
  NumericVector out(n);
  
  for (int i = 0; i < n; ++i) {
    double xi = x[i];
    if (xi < a) {
      out[i] = a;
    } else if (xi > b) {
      out[i] = b;
    } else {
      out[i] = xi;
    }
  }
  
  return out;
}")

microbenchmark(
  squish_in_place(x, -1, 1),
  squish_cpp(x, -1, 1),
  unit = "us"
)
```

It's often relatively easy to speed up the performance of R code just by making a few small changes (you'll learn more about that in the next chapter). Most package developers are not professional software engineers and are not paid to work on R packages. This means that most package code is written to solve a pressing problem. The vast majority of R code is poorly written and slow. This sounds bad but it's actually a reflection that people are doing real work, not messing around with code. R code can be incredibly inefficient but still fast enough. 

### Exercises

* The performance characteristics of `trunc_ife()`, `trunc_p()` and `trunc_in_place()` vary considerably with the size of `x`. Explore the differences. For what sizes of input is the difference biggest? Smallest?

* Compare the performance costs of extract an element from a list, a column from a matrix, and a column from a data frame. What about a row?

* `scan()` has the most arguments (21) of any base function. About how much time does it take to make 21 promises each time scan is called? Given a simple input (e.g. `scan(text = "1 2 3", quiet = T)`) what proportion of the total run time is due to creating those promises?

## How might R become faster? {#faster-r}

There are some exciting new projects that explore ways to make R faster. They all try to stick as closely as possible to the existing language definition, but implement ideas from modern compiler design to make the implementation as fast as possible. The three most mature projects are:

* [Pretty quick R](http://www.pqr-project.org/). By Radford Neal. Built on top of existing R code base (2.15.0).  Fixes many obvious performance issues. Better memory management.

* [Renjin](http://www.renjin.org/). JVM. Extensive [test suite](http://packages.renjin.org/). Good discussion at http://www.renjin.org/blog/2013-07-30-deep-dive-vector-pipeliner.html. Along the same lines, but currently less developed in [fastr](https://github.com/allr/fastr).  Written by the same group as traceR paper.

* [Riposte](https://github.com/jtalbot/riposte). Experimental VM. (http://www.justintalbot.com/wp-content/uploads/2012/10/pact080talbot.pdf)

These are roughly ordered in from most practical to most ambitious. There is one other project that currently does not provide any performance improvements, but might provide a better foundation for future improvements:

* [CXXR](http://www.cs.kent.ac.uk/projects/cxxr/). Reimplementation of R into clean C++. Not currently faster, but is much more extensible and might form clean foundation for future work. Better documentation of intenrals.  http://www.cs.kent.ac.uk/projects/cxxr/pubs/WIREs2012_preprint.pdf. Behaviour identical.

R is a huge language and it's not clear whether any of these approaches will even become mainstream. It's hard task to make sure an alternative backend can run all R code in the same way (or similar enough) to GNU R. The challenge with any rewrite is maintaining compatibility with base R. Can you imagine having to reimplement every function in base R to not only be faster, but to have exactly the same documented bugs? (e.g. `nchar(NA)`).

However, even if these implementations never make a dent in the use of GNU R, they have other benefits:

* Simpler implementations make it easy to validate new approaches before
  porting to the GNU R.
  
* Gain understanding about which currently core features could be changed
  with minimal changes to existing code and maximal impact on performance.
  
* Alternative implementations put pressure on the R Core Team to incorporate
  performance improvements.

One of the most important approaches that pqr, renjin and riposte are exploring is the idea of deferred evaluation. As Justin Talbot, the author of riposte, points out "for long vectors, R's execution is completely memory bound. It spends almost all of its time reading and writing vector intermediates to memory". If you can eliminate intermediate vectors, you can not only decrease memory usage, you can also considerably improve performance.

The following example shows a very simple example of where deferred evaluation might help. We have three vectors, `x`, `y`, `z` each containing 1 million elements, and we want to find the average value of `x` + `y` where `z` is TRUE. (This represents a simplification of a pretty common sort of data analysis question.)

```{r}
x <- runif(1e6)
y <- runif(1e6)
z <- sample(c(T, F), 1e6, rep = TRUE)

mean((x + y)[z])
```

In R, this creates two big temporary vectors: `x + y`, 1 million elements long, and `(x + y)[z]`, about 500,000 elements long. This means you need to have extra memory free for the intermediate calculation, and allocating and freeing that memory is relatively expensive so it slows the computation down.

If we rewrote the function using a loop in a language like C++, we could recognise that we only need one intermediate value: the sum of all the values we've seen:

```{r, engine = "Rcpp"}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double cond_mean_cpp(NumericVector x, NumericVector y, LogicalVector z) {
  double sum = 0;
  int n = x.length();
  
  for(int i = 0; i++; i < n) {
    if (!z[i]) continue;
    sum += x[i] + y[i];
  }
  
  return sum / n;
}
```

On my computer, this approach is about 8 times faster than the vectorised R equivalent (which is already pretty fast).

```{r}
cond_mean_r <- function(x, y, z) {
  mean((x + y)[z])
}

microbenchmark(
  cond_mean_cpp(x, y, z),
  cond_mean_r(x, y, z)
)
```

Riposte, renjin and pqr all provide tools to do this sort of transformation automatically, so you can write the concise R and it's automatically translated into efficient machine code.  Sophisticated translators can also figure out how to make the most of multiple cores. In the above example, if you have four cores, you could give 1/4 of the vectors to each core and then add together the results, possibly giving a 4-fold speed up.


