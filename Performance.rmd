---
title: Performance
layout: default
---

# Performance

<!--
Once more complete, need to circulate to Justin, Radford, Alex & Jan for comments.
-->

There's no getting away from the fact that as a computer language, R is relatively slow. In this chapter we'll explore some of the reasons why R is slow and learn about microbenchmarking as a tool to understand R's performance characterisics. I'll also talk about some of the projects that aim to overcome R's shortcomigns and speculate a little on the future. By the end of this chapter you'll have a better understanding of why R is slow. The next chapter will focus on what you can do about it for your code.

Absolute vs. relative speed. Fast vs. fast enough.



## Why is R slow?

General advice from https://www.cs.purdue.edu/homes/jv/pubs/ecoop12.pdf

Fundamental reasons for R's slowness

* Missing value support
* Many inner workings are exposed: first class environments, promises, ...
* Promises are usually executed right away, but have overhead associated with them.
* Everything is a lexically scoped function call 
* Dynamic typing
* Everything can be modified after the fact. After function has been defined, can change it's body, arguments and environment
* No scalars

It's not possible to make code to implement this fast, but the complicated semantics and the fact that you can override the default behaviour of almost anything makes it very hard. It seems likely that R could probably be made 10x faster (and use much less memory) with a reasonable effort. 

The challenge with any rewrite is maintaining compatibility with base R. Can you imagine having to reimplement every function in base R to not only be faster, but to have exactly the same documented bugs? (e.g. `nchar(NA)`)

Incidental reasons for R's slownesss

* Deferred evaluation: x + y * z. http://radfordneal.wordpress.com/2013/07/24/deferred-evaluation-in-renjin-riposte-and-pqr/. JT: "for long vectors, R’s execution is completely memory bound. It spends almost all of its time reading and writing vector intermediates to memory"
* Many R core functions not implemented with goal of being fast
* Most R users are not programmers, so mistakes in common code and packages.
* Base library missing important data structures
* Technical debt. A no point has anyone been paid to work on making R faster or cleaner

While R will get faster over time as incidental reasons for poor performance, R is never going to be a fast language. It's never going to be as fast as C, or probably even as fast as javascript (unless someone spends a lot of money). That's because R's design is not optimised for performance, but instead it's optimised for expressiveness.  John Chambers, the designer of the S language that underlies R, recognised that the biggest bottleneck in most data analyses is congitive, not computational. It takes much longer to figure out what you want do, and express it 

## R futures

* [Pretty quick R](http://www.pqr-project.org/)
* [Riposte](http://www.justintalbot.com/wp-content/uploads/2012/10/pact080talbot.pdf)
* [Renjin](http://www.renjin.org/)
* [fastr](https://github.com/allr/fastr)
* [CXXR](http://www.cs.kent.ac.uk/projects/cxxr/)

R is slow, but most of the time that doesn't matter. Cognition time >> computation time. Many R functions are inefficient. Can often recreate slow general purpose functions to fast special purpose functions. Can easily access faster languages.

## Micro-benchmarking

Once you have identified the performance bottleneck in your code, you'll want to try out many variant approaches.

The [microbenchmark][microbenchmark] package is much more precise than `system.time()` with nanosecond rather than millisecond precision. This makes it much easier to compare operations that only take a small amount of time. For example, we can determine the overhead of calling a function: (for an example in the package)

```{r}
library(microbenchmark)

f <- function() NULL
microbenchmark(
  NULL,
  f()
)
```

It's about ~150 ns on my computer (that's the time taken to set up the new environment for the function etc). 

It's hard to accurately compute this difference with `system.time` because we need to repeat the operation about a million times, and we get no information about the variability of the estimate.  The results may also be systematically biased if some other computation is happening in the background during one of the runs.
  
```{r, cache = TRUE}
x <- 1:1e6
system.time(for (i in x) NULL) * 1e3
system.time(for (i in x) f()) * 1e3
```

Running both examples on my computer a few times reveals that the estimate from `system.time` is about 20 nanoseconds higher than the median from `microbenchmark`.

By default, microbenchmark evaluates each expression 100 times, and in random order to control for any systematic variability. It also provides times each expression individually, so you get a distribution of times, which helps estimate error.  You can also display the results visually using either `boxplot()`, or if you have `ggplot2` loaded, `autoplot()`:

```{r microbenchmark}
f <- function() NULL
g <- function() f()
h <- function() g()
i <- function() h()
m <- microbenchmark(
  NULL,
  f(), 
  g(),
  h(),
  i())
boxplot(m)
library(ggplot2)
autoplot(m)
```

Microbenchmarking allows you to take the very small parts of a program that profiling has identified as being bottlenecks and explore alternative approaches.  It is easier to do this with very small parts of a program because you can rapidly try out alternatives without having to worry too much about correctness (i.e. you are comparing alternatives that are so simple it's obvious whether they're correct or not.)

Useful to think about the first part of the process, generating possible alternatives as brainstorming.  You want to come up with as many different approaches to the problem as possible.  Don't worry if some of the approaches seem like they will _obviously_ be slow: you might be wrong, or that approach might be one step towards a better approach.  To get out of a local maxima, you must go down hill.

When doing microbenchmarking, you not only need to figure out what the best method is now, but you need to make sure that fact is recorded somewhere so that when you come back to the code in the future, you remember your reasoning and don't have to redo it again. I find it really useful to write microbenchmarking code as Rmarkdown documents so that I can easily integrate the benchmarking code as well as text describing my hypotheses about why one method is better than another, and listing things that I tried that weren't so effective.

Microbenchmarking is also a powerful tool to improve your intuition about what operations in R are fast and what are slow. The following XXX examples show how to use microbenchmarking to determine the costs of some common R actions, but I really recommend setting up some experiments for the R functions that you use most commonly.

* What's the cost of function vs S3 or S4 method dispatch? 
* What's the fastest way to extract a column out of data.frame?

### Generating promises

```{r promise-cost}
f1 <- function(a = 1) NULL
f2 <- function(a = 1, b = 1) NULL
f3 <- function(a = 1, b = 2, c = 3) NULL
f4 <- function(a = 1, b = 2, c = 4, d = 4) NULL
f5 <- function(a = 1, b = 2, c = 4, d = 4, e = 5) NULL
microbenchmark(
  f1(),
  f2(),
  f3(),
  f4(), 
  f5(), 
  times = 1000
)
```

### Method dispatch

The following microbenchmark compares the cost of generating one uniform number directly, with a function, with a S3 method, with a S4 method and a R5 


```{r}
f <- function(x) NULL

s3 <- function(x) UseMethod("s3")
s3.integer <- function(x) NULL

A <- setClass("A", representation(a = "list"))
setGeneric("s4", function(x) standardGeneric("s4"))
setMethod(s4, "A", function(x) NULL)

B <- setRefClass("B")
B$methods(r5 = function(x) NULL)

a <- A()
b <- B$new()

microbenchmark(
  bare = NULL,
  fun = f(),
  s3 = s3(1L),
  s4 = s4(a),
  r5 = b$r5()
)
```

On my computer, the bare call takes about 40 ns. Wrapping it in a function adds about an extra 200 ns - this is the cost of creating the environment where the function execution happens. S3 method dispatch adds around 3 µs and S4 around 12 µs.

However, it's important to notice the units: microseconds. There are a million microseconds in a second, so it will take hundreds of thousands of calls before the cost of S3 or S4 dispatch appreciable. Most problems don't involve hundreds of thousands of function calls, so it's unlikely to be a bottleneck in practice.This is why microbenchmarks can not be considered in isolation: they must be  carefully considered in the context of your real problem.

### Extracting variables out of a data frame

For the plyr package, I did a lot of experimentation to figure out the fastest way of extracting data out of a data frame.

```{r}
n <- 1e5
df <- data.frame(matrix(runif(n * 100), ncol = 100))
x <- df[[1]]
x_ran <- sample(n, 1e3)

microbenchmark(
  x[x_ran],
  df[x_ran, 1],
  df[[1]][x_ran],
  df$X1[x_ran],
  df[["X1"]][x_ran],
  .subset2(df, 1)[x_ran],
  .subset2(df, "X1")[x_ran]
)
```
Again, the units are in microseconds, so you only need to care if you're doing hundreds of thousands of data frame subsets - but for plyr I am doing that so I do care.

### Vectorised operations on a data frame


```{r}
df <- data.frame(a = 1:10, b = -(1:10))
l <- list(0, 10)
l_2 <- list(rep(0, 10), rep(10, 10))
m <- matrix(c(0, 10), ncol = 2, nrow = 10, byrow = TRUE)
df_2 <- as.data.frame(m)
v <- as.numeric(m)

microbenchmark(
  df + v,
  df + l,
  df + l_2,
  df + m,
  df + df_2
)
```


## Brainstorming

Most important step is to brainstorm as many possible alternative approaches.

Good to have a variety of approaches to call upon.  

* Read blogs
* Algorithm/data structure courses (https://www.coursera.org/course/algs4partI)
* Book
* Read R code

We introduce a few at a high-level in the Rcpp chapter.

[microbenchmark]: http://cran.r-project.org/web/packages/microbenchmark/index.html
