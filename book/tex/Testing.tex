\chapter{Testing}

To make sure that your package behaves as you'd expect, testing is
vital. While you probably test your code already, you may not have taken
the next step: automation. This chapter describes how to use the
\texttt{testthat} package to create automated tests of your code.

\section{Motivation}

I started automating my tests because I discovered I was spending too
much time recreating bugs that I had previously fixed. While writing
code or fixing bugs, I'd perform interactive tests to make sure the code
worked. But I never had a system which could store those test so I could
re-run them as needed. I think that this is a common practice among R
programmers. It's not that we don't test our code, it's that we don't
have a way to make it easy to re-run tests, let alone to do so
automatically.

Turning your casual interactive tests into reproducible scripts requires
a little more work up front. Since you can no longer visually inspect
the output, you have to write code that does the inspection for you.
However, this is an investment in the future of your code. It pays off
in four ways:

\begin{itemize}
\item
  Decreased frustration. Whenever I'm on a strict deadline I always seem
  to discover a bug in old code. Having to stop what I'm doing to fix it
  is a real pain. The more I test, the less this happens. Also, based on
  how well they test, I can easily see which parts of my code I can be
  confident about.
\item
  Better code structure. Code that's easy to test is usually better
  designed. I have found writing tests makes me break up complicated
  parts of my code into separate functions that can work in isolation.
  These functions have less duplication, and are easier to test,
  understand and re-combine.
\item
  Easier to pick up where you left off. If you always finish a coding
  session by creating a failing test (e.g.~for the feature you want to
  implement next), testing makes it easier to pick up where you left
  off: your tests let you know what to do next.
\item
  Increased confidence when making changes. If you know that all major
  functionality has an associated test, you can confidently make big
  changes without worrying about accidentally breaking something. For
  me, this is particularly useful when I think of a simpler way to
  accomplish a task: often my simpler solution is only simpler because
  I've forgotten an important use case!
\end{itemize}

\section{testthat test structure}

\texttt{testthat} has a hierarchical structure made up of expectations,
tests and contexts.

\begin{itemize}
\item
  An \textbf{expectation} describes the expected result of a
  computation: Does it have the right value and right class? Does it
  produce error messages when it should? There are 11 types of built in
  expectations.
\item
  A \textbf{test} groups together multiple expectations to test one
  function, or tightly related functionality across multiple functions.
  A test is created with the \texttt{test\_that()} function.
\item
  A \textbf{context} groups together multiple tests that test related
  functionality. Contexts are defined with the \texttt{context()}
  function.
\end{itemize}

These are described in detail below.

Expectations give you the tools to convert your visual, interactive
experiments into reproducible scripts. Tests and contexts are ways of
organising your expectations so that when something goes wrong you can
easily track down the source of the problem.

\subsection{Expectations}

An expectation is the finest level of testing. It makes a binary
assertion about whether or not a value is as you expect. If the
expectation isn't true, \texttt{testthat} will raise an error.

An expectation is easy to read. Its syntax is close to a sentence in
English: \texttt{expect\_that(a, equals(b))} reads as ``I expect that a
will equal b''.

There are 11 built in expectations:

\begin{itemize}
\item
  \texttt{equals()} uses \texttt{all.equal()} to check for equality
  within some numerical tolerance.

  \# Passes expect\_that(10, equals(10)) \# Also passes expect\_that(10,
  equals(10 + 1e-7))\\ \# Fails expect\_that(10, equals(10 + 1e-6))\\ \#
  Definitely fails! expect\_that(10, equals(11))
\item
  \texttt{is\_identical\_to()} uses \texttt{identical()} to check for
  exact equality.

  \# Passes expect\_that(10, is\_identical\_to(10)) \# Fails
  expect\_that(10, is\_identical\_to(10 + 1e-10))
\item
  \texttt{is\_equivalent\_to()} is a more relaxed version of
  \texttt{equals()} that ignores attributes:

  \# Fails expect\_that(c(``one'' = 1, ``two'' = 2), equals(1:2)) \#
  Passes expect\_that(c(``one'' = 1, ``two'' = 2),
  is\_equivalent\_to(1:2))
\item
  \texttt{is\_a()} checks that an object \texttt{inherit()}s from a
  specified class.

  model \textless{}- lm(mpg \textasciitilde{} wt, data = mtcars) \#
  Passes expect\_that(model, is\_a(``lm''))\\ \# Fails
  expect\_that(model, is\_a(``glm''))
\item
  \texttt{matches()} matches a character vector against a regular
  expression. The optional \texttt{all} argument controls whether all
  elements or just one element needs to match. This code is powered by
  \texttt{str\_detect()} from the \texttt{stringr} package.

  string \textless{}- ``Testing is fun!'' \# Passes expect\_that(string,
  matches(``Testing'')) \# Fails, match is case-sensitive
  expect\_that(string, matches(``testing'')) \# Passes, match can be a
  regular expression expect\_that(string, matches(``t.+ting''))
\item
  \texttt{prints\_text()} matches the printed output from an expression
  against a regular expression.

  a \textless{}- list(1:10, letters) \# Passes expect\_that(str(a),
  prints\_text(``List of 2'')) \# Passes expect\_that(str(a),
  prints\_text(fixed(``int {[}1:10{]}''))
\item
  \texttt{shows\_message()} checks that an expression shows a message:

  \# Passes expect\_that(library(mgcv), shows\_message(``This is
  mgcv''))
\item
  \texttt{gives\_warning()} expects that you get a warning.

  \# Passes expect\_that(log(-1), gives\_warning())
  expect\_that(log(-1), gives\_warning(``NaNs produced'')) \# Fails
  expect\_that(log(0), gives\_warning())
\item
  \texttt{throws\_error()} verifies that the expression throws an error.
  You can also supply a regular expression which is applied to the text
  of the error.

  \# Fails expect\_that(1 / 2, throws\_error()) \# Passes expect\_that(1
  / ``a'', throws\_error()) \# But better to be explicit expect\_that(1
  / ``a'', throws\_error(``non-numeric argument''))
\item
  \texttt{is\_true()} is a useful catchall if none of the other
  expectations do what you want - it checks that an expression is true.
  \texttt{is\_false()} is the complement of \texttt{is\_true()}.
\end{itemize}

If you don't like the readable, but verbose, \texttt{expect\_that}
style, you can use one of the shortcut functions:

Full

Abbreviation

expect\_that(x, is\_true())

expect\_true(x)

expect\_that(x, is\_false())

expect\_false(x)

expect\_that(x, is\_a(y))

expect\_is(x, y)

expect\_that(x, equals(y))

expect\_equal(x, y)

expect\_that(x, is\_equivalent\_to(y))

expect\_equivalent(x, y)

expect\_that(x, is\_identical\_to(y))

expect\_identical(x, y)

expect\_that(x, matches(y))

expect\_match(x, y)

expect\_that(x, prints\_text(y))

expect\_output(x, y)

expect\_that(x, shows\_message(y))

expect\_message(x, y)

expect\_that(x, gives\_warning(y))

expect\_warning(x, y)

expect\_that(x, throws\_error(y))

expect\_error(x, y)

Running a sequence of expectations is useful because it ensures that
your code behaves as expected. You could even use an expectation within
a function to check that the inputs are what you expect. However,
they're not so useful when something goes wrong: all you know is that
something is not as expected. You don't know anything about where the
problem is. Tests, described next, organise expectations into coherent
blocks that describe the overall goal of a set of expectations.

\subsection{Tests}

Each test should test a single item of functionality and have an
informative name. The idea is that when a test fails, you should know
exactly where to look for the problem in your code. You create a new
test with \texttt{test\_that()}, with parameters name and code block.
The test name should complete the sentence ``Test that'' and the code
block should be a collection of expectations. When there's a failure,
it's the test name that will help you figure out what's gone wrong.

The following code shows one test of the \texttt{floor\_date()} function
from \texttt{library(lubridate)}. There are 7 expectations that check
the results of rounding a date down to the nearest second, minute, hour,
etc. Note how we've defined a couple of helper functions to make the
test more concise so you can easily see what changes in each
expectation.

\begin{verbatim}
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

  is_time <- function(x) equals(as.POSIXct(x, tz = "UTC"))
  floor_base <- function(unit) floor_date(base, unit)

  expect_that(floor_base("second"), is_time("2009-08-03 12:01:59"))
  expect_that(floor_base("minute"), is_time("2009-08-03 12:01:00"))
  expect_that(floor_base("hour"),   is_time("2009-08-03 12:00:00"))
  expect_that(floor_base("day"),    is_time("2009-08-03 00:00:00"))
  expect_that(floor_base("week"),   is_time("2009-08-02 00:00:00"))
  expect_that(floor_base("month"),  is_time("2009-08-01 00:00:00"))
  expect_that(floor_base("year"),   is_time("2009-01-01 00:00:00"))
})
\end{verbatim}

Each test is run in its own environment so it is self-contained. The
exceptions are actions which have effects outside the local environment.
These include things that affect:

\begin{itemize}
\item
  the filesystem: creating and deleting files, changing the working
  directory, etc.
\item
  the search path: package loading \& detaching, \{\tt attach\}.
\item
  global options, like \texttt{options()} and \texttt{par()}.
\end{itemize}

When you use these actions in tests, you'll need to clean up after
yourself. Many other testing packages have set-up and teardown methods
that are run automatically before and after each test. These are not so
important with \texttt{testthat} because you can create objects outside
of the tests and rely on R's copy-on-modify semantics to keep them
unchanged between test runs. To clean up other actions you can use
regular R functions.

You can run a set of tests just by \texttt{source()}ing a file, but as
you write more and more tests, you'll probably want a little more
infrastructure. The first part of that infrastructure is contexts,
described below, which give a convenient way to label each file, helping
to locate failures when you have many tests.

\section{Contexts}

Contexts group tests together into blocks that test related
functionality, and are established with the code
\texttt{context("My context")}. Normally there is one context per file,
but you can have more if you want, or you can use the same context in
multiple files.

The following code shows the context that tests the operation of the
\texttt{str\_length()} function in \texttt{stringr}. The tests are very
simple. They cover two situations where \texttt{nchar()} from base R
gives surprising results.

\begin{verbatim}
context("String length")

test_that("str_length is number of characters", {
  expect_that(str_length("a"), equals(1))
  expect_that(str_length("ab"), equals(2))
  expect_that(str_length("abc"), equals(3))
})

test_that("str_length of missing is missing", {
  expect_that(str_length(NA), equals(NA_integer_))
  expect_that(str_length(c(NA, 1)), equals(c(NA, 1)))
  expect_that(str_length("NA"), equals(2))
})

test_that("str_length of factor is length of level", {
  expect_that(str_length(factor("a")), equals(1))
  expect_that(str_length(factor("ab")), equals(2))
  expect_that(str_length(factor("abc")), equals(3))
})
\end{verbatim}

\section{Running tests}

There are two situations where you want to run your tests: interactively
while you're developing your package to make sure that everything works
ok, and then as a final automated check before releasing your package.

\begin{itemize}
\item
  run all tests in a file or directory \texttt{test\_file()} or
  \texttt{test\_dir()}
\item
  automatically run tests whenever something changes with
  \texttt{auto\_test}.
\item
  have \texttt{R CMD check} run your tests.
\end{itemize}

\subsection{Testing files and directories}

You can run all tests in a file with \texttt{test\_file(path)}. The
following code shows the difference between \texttt{test\_file} and
\texttt{source} for the \texttt{stringr} tests, as well as those same
tests for \texttt{nchar}. You can see the advantage of
\texttt{test\_file} over \texttt{source}: instead of seeing the first
failure, you see the performance of all tests.

\begin{verbatim}
> source("test-str_length.r")
> test_file("test-str_length.r")
.........

> source("test-nchar.r")
Error: Test failure in 'nchar of missing is missing'
* nchar(NA) not equal to NA_integer_
'is.NA' value mismatch: 0 in current 1 in target
* nchar(c(NA, 1)) not equal to c(NA, 1)
'is.NA' value mismatch: 0 in current 1 in target

> test_file("test-nchar.r")
...12..34

1. Failure: nchar of missing is missing ---------------------------------
nchar(NA) not equal to NA_integer_
'is.NA' value mismatch: 0 in current 1 in target

2. Failure: nchar of missing is missing ---------------------------------
nchar(c(NA, 1)) not equal to c(NA, 1)
'is.NA' value mismatch: 0 in current 1 in target

3. Failure: nchar of factor is length of level --------------------------
nchar(factor("ab")) not equal to 2
Mean relative difference: 0.5

4. Failure: nchar of factor is length of level --------------------------
nchar(factor("abc")) not equal to 3
Mean relative difference: 0.6666667
\end{verbatim}

Each expectation is displayed as either a green dot (indicating success)
or a red number (including failure). That number indexes a list of
further details, which is printed after all tests have been run. What
you can't see is that this display is dynamic: a new dot is printed each
time a test passes (it's rather satisfying to watch).

\texttt{test\_dir} will run all of the test files in a directory,
assuming that test files start with \texttt{test} (this means it's
possible to intermix regular code and tests in the same directory). This
is handy if you're developing a small set of scripts rather than a
complete package. The following shows the output from the
\texttt{stringr} tests. You can see there are 12 contexts with between 2
and 25 expectations each. As you'd hope for in a released package, all
the tests pass.

\begin{verbatim}
> test_dir("inst/tests/")
String and pattern checks : ......
Detecting patterns : .........
Duplicating strings : ......
Extract patterns : ..
Joining strings : ......
String length : .........
Locations : ............
Matching groups : ..............
Test padding : ....
Splitting strings : .........................
Extracting substrings : ...................
Trimming strings : ........
\end{verbatim}

If you want a more minimal report, suitable for display on a dashboard,
you can use a different reporter. \texttt{testthat} comes with three
reporters: stop, minimal and summary. The stop reporter is the default
and \texttt{stop()}s whenever a failure is encountered. The summary
reporter is the default for \texttt{test\_file} and \texttt{test\_dir}.
The minimal reporter is shown below: it prints \texttt{.} for success,
\texttt{E} for error and \texttt{F} for failure. The following output
shows the results of running the \texttt{stringr} test suite with the
minimal reporter.

\begin{verbatim}
> test_dir("inst/tests/", reporter="minimal")
...............................................
\end{verbatim}

\subsection{Autotest}

Tests are most useful when run frequently, and \texttt{auto\_test} takes
that idea to the limit by rerunning your tests whenever your code or
tests changes. \texttt{auto\_test()} has two arguments,
\texttt{code\_path} and \texttt{test\_path}, which point to a directory
of source code and tests respectively.

Once run, \texttt{auto\_test()} will continuously scan both directories
for changes. If a test file is modified, it will test that file. If a
code file is modified, it will reload that file and rerun all tests. To
quit, you'll need to press Ctrl + Break in Windows, Esc in Mac OS, or
Ctrl + C if running from the command line.

This promotes a workflow where the \emph{only} way you test your code is
through tests. Instead of modify-save-source-check you just modify and
save, then watch the automated test output for problems.

\section{R CMD check}

If you're using \texttt{testthat} in a package, you need to adopt a
specific structure to work with \texttt{R CMD check}.

First, you need to add \texttt{Suggests: testthat} to
\texttt{DESCRIPTION} as stated in the
\href{http://cran.r-project.org/doc/manuals/R-exts.html\#Package-Dependencies}{Writing
R Extensions} document. This avoids a R CMD check warning about
unspecified dependencies.

After that, you need to put the tests somewhere \texttt{R CMD check}
will find and run them.

Previously, best practice was to put all test files in
\texttt{inst/tests} and ensure that R CMD check ran them by putting the
following code in \texttt{tests/test-all.R}:

\begin{verbatim}
library(testthat)
library(yourpackage)
test_package("yourpackage")
\end{verbatim}

Now, recommended practice is to put your tests in
\texttt{tests/testthat}, and ensure R CMD check runs them by putting the
following code in \texttt{tests/test-all.R}:

\begin{verbatim}
library(testthat)
test_check("yourpackage")
\end{verbatim}

The advantage of this new structure is that the user has control over
whether or not tests are installed using the --install-tests parameter
to R CMD install, or INSTALL\_opts = c(``--install-tests'') argument to
install.packages(). I'm not sure why you wouldn't want to install the
tests, but now you have the flexibility as requested by CRAN
maintainers.
