---
title: Memory usage
layout: default
---

# Memory

It's important to understand memory usage in R, firstly because you might be running out, and secondly because efficiently managing memory can make your code faster. The goals of this chapter are:

* to give you a basic understanding of how memory works in R, by explaining `object.size()`

* to help you predict when a object will be copied, show you how to test your prediction, and give you some tips for avoiding copies

* give you practical practical tools to understand memory allocation in a given problem

* build your vocabulary so you can more easily understand more advanced documentation.

Unfortunately the details of memory management in R is not documented in one place, but most of the information in this chapter I cleaned from close reading of the documentation (partiularly `?Memory` and `?gc`), the [memory profiling](http://cran.r-project.org/doc/manuals/R-exts.html#Profiling-R-code-for-memory-use) section of R-exts, and the [SEXPs](http://cran.r-project.org/doc/manuals/R-ints.html#SEXPs) section of R-ints. The rest I figured out by small experiments and by asking questions on R-devel. 

## `object.size()`

One of the most useful tools for understanding memory usage in R is `object.size()`. It's analogous to `system.time()`, and tells you how much memory an R object occupies. This section explores the usage of `object.size()` and by explaining some unusual findings, will help you understand some important aspects of memory allocation.

We'll start with a suprising plot:  a plot of vector length vs. the number of bytes of memory it occupies. You might have expected that the size of an empty vector would be 0 and that the memory usage would grow proportionately with length. Neither of those things are true!

```{r size-q}
sizes <- sapply(0:50, function(n) object.size(seq_len(n)))
plot(0:50, sizes, xlab = "Length", ylab = "Bytes", type = "s")
```

It's not just numeric vectors of length 0 that occupy 40 bytes of memory, it's every empty vector type:

```{r}
object.size(numeric())
object.size(integer())
object.size(raw())
object.size(list())
```

What are those 40 bytes of memory used for? There are four components that every object in R has:

* 4 bytes: object metadata, the _sxpinfo_. This metadata includes the base type, and information used for debugging and memory management.

* 2 * 8 bytes: two pointers needed for memory management. Objects in R are stored in a doubly-linked list, so that R can easily iterate through every object stored in memory.

* 8 bytes: a pointer to the attributes.

And three components possessed by all vector types:

* 4 bytes: the length of the vector. Using 4 bytes means that R could previously only support vectors up to 2 ^ 31 - 1 (about two billion) elements long. You can read in R-internals about how support for [long vectors](http://cran.r-project.org/doc/manuals/R-ints.html#Long-vectors) was added in R 3.0.0, without changing the size of this field.

* 4 bytes: the "true" length, which is basically never used (the exception is for environments with hastables, where the hashtable is a list, and the truelength represents the allocated space and length represents the space)

* ?? bytes: the data. An empty vector has 0 bytes of data, but it's obviously very important otherwise!

If you're counting closely you'll note that only this adds up to 36 bytes. The other 4 bytes are needed as padding after the sxpinfo, so the pointers start on 8 bytes (=64-bit) boundaries, which is needed by most architectures (and for the ones it's not required, accessing non-aligned pointers tends to be rather slow). 

That explains the intercept on the graph. But why does the memory size grow in irregular jumps? To understand that, you need to know a little bit about how R requests memory from the operating system. Requesting memory, using the `malloc()` function, is a relatively expensive operation, and it would make R slow if it had to request memory every time you created a little vector.  Instead, it asks for a big block of memory and then manages it itself: this is called the small vector pool. R uses this pool for vectors less than 128 bytes long, and for efficiency and simplicitly reasons, only allocates vectors that are 8, 16, 32, 48, 64 or 128 bytes long. If we adjust our previous plot by removing the 40 bytes of overhead we can see that those values correspond to the jumps.

```{r size-a}
plot(0:50, sizes - 40, xlab = "Length", ylab = "Bytes excluding overhead", type = "n")
abline(h = 0, col = "grey80")
abline(h = c(8, 16, 32, 48, 64, 128), col = "grey80")
abline(a = 0, b = 4, col = "grey90", lwd = 4)
lines(sizes - 40, type = "s")
```

It only remains to explain the jumps after the 128 limit. While it makes sense for R to manage it's own memory for small vectors, it doesn't make sense to manage it for large vectors: allocating big chunks of memory is something that operating systems are very good at. R always asks for memory in multiples of 8 bytes: this ensures good alignment for the data, in the same way we needed good alignment for the pointers described above.

There are a few other subtleties to `object.size()`:  it only promises to give an estimate of the memory usage, not the actual usage. This is because for more complex objects it's not immediately obvious what memory memory usage means. Take environments for example, using `object.size()` on an environment tells you the size of the environment, not the total size of its contents. It would be easy to create a function that did this:

```{r}
obj_size <- function(x) {
  if (!is.environment(x)) return(object.size(x))
  
  objs <- ls(x, all = TRUE) 
  sizes <- vapply(objs, function(o) obj_size(get(o, x)), double(1))
  structure(sum(sizes), class = "object_size")
}
object.size(environment())
obj_size(environment())
```

However, it's difficult to do this correctly using R code, because there are a lot of special cases that you need to recurse through (for example, you might have an object with an attribute that's a function with an environment that contains a large object.)

There's a good reason for this - it's not immediately obvious how much space an environment takes up because environment objects are reference based.  In the following example, what is the size of `a1`? What is the size of `a2`? 

```{r}
e <- new.env()
e$x <- 1:1e6

a1 <- list(e)
object.size(a1)
a2 <- list(e)
object.size(a2)
```

Another challenge for `object.size()` is strings: 

```{r}
object.size("banana")
object.size(rep("banana", 100))
```

On my 64-bit computer, the size of a vector containing "banana" is 96 bytes, but the size of a vector containing 100 "banana"s is 888 bytes. Why the difference? The key is 888 = 96 + 99 * 8. R has a global string pool, which means that every unique string is only stored once in memory. Every other instance of that string is just a pointer, and only needs 8 bytes of storage.

## Total memory use

`object.size()` tells you the size of a single object; `gc()` (among other things) tells you total size of all objects in memory:

```{r}
gc()
```

(we'll get to why it's called `gc()` in the next section)

R breaks down memory usage into Vcells (memory used by vectors) and Ncells (memory used by everything else). However, this distinction isn't usually important, and neither are the gc trigger and max used columns. What you're usually most interested in is the total memory used. The function below wraps up `gc()` to just return the number of megabytes of memory you're currently using.

```{r}
mem <- function() {
  bit <- 8L * .Machine$sizeof.pointer
  if (!(bit == 32L || bit == 64L)) {
    stop("Unknown architecture", call. = FALSE)
  }
  
  node_size <- if (bit == 32L) 28L else 56L
  
  usage <- gc()
  sum(usage[, 1] * c(node_size, 8)) / (1024 ^ 2)
}
mem()
```

Don't expect this number to agree with the amount of memory that your operating system says that R is using:

* There is some overhead associated with the R interpreter that is not captured by these numbers, 

* both R and the operating system are lazy: they won't try and reclaim memory until it's actually needed. 

* R counts the memory occupied by objects; there may be gaps from objects that have been deleted. This problem is known as memory fragmentation.

We can build a function of top of `mem()` that tells us how memory changes during the execution of a block of code. We use a little special evaluation to make the code behave just as if we'd run it directly.

```{r}
mem_change <- function(code) {
  start <- mem()
  
  expr <- substitute(code)
  eval(expr, parent.frame())
  rm(code, expr)
  
  round(mem() - start, 3)
}
# Need about 4 mb to store 1 million integers
mem_change(x <- 1:1e6)
# We get that memory back when we delete it
mem_change(rm(x))
```

In the next section, we'll use `mem_change()` to explore how memory is allocated and released by R, and memory is released lazily by the "garbage collector".

## Garbarge collection

In some languages you have to explicitly delete unnused objects so that their memory can be returned. R uses an alternative approach, called garbage collection (GC for short), which automatically released memory when an object is no longer used. It does this based on environments and the regular scoping rules: when an environment goes out of scope (for example, when a function finishes executing), all of the contents of that environment are deleted and their memory is freed.

For example, in the following code, a million integers are allocated inside the function, but are automatically cleaned up when the function terminates. This results in a net change of zero:

```{r}
f <- function() {
  1:1e6
}
mem_change(f())
```

Unfortunately we're not seeing a completely accurate picture of normally memory usage, because in order to find out how much memory is available, we call `gc()`. As well as returning the amount of memory currently used, `gc()` also triggers garbage collection. Garbage collection normally happens lazily: R calls `gc()` when it needs more space. In reality, that R might hold onto the memory after the function has terminated, but it will release it as soon as it's needed.

Despite what you might have read elsewhere, there's never any point in calling `gc()` yourself, apart to see how much memory is in use. R will automatically call run garbage collection whenever it needs more space; if you want to see when that is, call `gcinfo(TRUE)`. The only reason you _might_ want to call `gc()` is that it also requests that R should return memory to the operating system. Even that might not have any effect: older versions of Windows had no way for a program to return memory to the OS.

Generally, GC takes care of releasing previously used memory. However, you do need to be aware of situations that can cause memory leaks: when you think you've removed all references to an object, but some are still hanging around so the object never gets freed. In R, the two main causes of memory leaks are formulas and closures. They both capture the enclosing environment, so any objects in that environment will not be reclaimed.

The following code illustrates the problem. `f1()` just returns the object `10`, so the large vector allocated inside the function will go out of scope and get reclaimed, and the net memory change is 0. `f2()` and `f3()` both return objects that capture environments, and so the net memory change is almost 4 megabytes.

```{r}
f1 <- function() {
  x <- 1:1e6
  10
}
mem_change(x <- f1())
x
rm(x)

f2 <- function() {
  x <- 1:1e6
  a ~ b
}
mem_change(y <- f2())
object.size(y)
rm(y)

f3 <- function() {
  x <- 1:1e6
  function() 10
}
mem_change(z <- f3())
object.size(z)
rm(z)
```

## Memory profiling with lineprof

As well as using `mem_change()` to explicitly capture the change in memory caused by running a block of code, we can use memory profiling to automatically capture memory usage every few milliseconds. This functionality is provided by the built in `Rprof()` function, but it doesn't provide a very useful visualisation of the output. Instead, we'll use the [lineprof](https://github.com/hadley/lineprof) package to explore the output.

To demonstrate lineprof, we're going to explore a minimal implementation of `read.delim` with only has three arguments:

```{r}
read_delim <- function(file, header = TRUE, sep = ",") {
  # Determine number of fields by reading first line
  first <- scan(file, what = character(1), nlines = 1, sep = sep, quiet = TRUE)
  p <- length(first)
  
  # Load all fields as character vectors
  all <- scan(file, what = as.list(rep("character", p)), sep = sep, 
    skip = if (header) 1 else 0, quiet = TRUE)
  
  # Convert from strings to appropriate types (never to factors)
  all[] <- lapply(all, type.convert, as.is = TRUE)
  
  # Set column names
  if (header) {
    names(all) <- first
  } else {
    names(all) <- paste0("V", seq_along(all))
  }
  
  # Convert list into data frame
  as.data.frame(all)
}
```

We'll also create a sample csv file to load in:

```{r}
library(ggplot2)
write.csv(diamonds, "diamonds.csv", row.names = FALSE)
```

Using lineprof is straightforward:

```{r, eval = FALSE}
library(lineprof)

source("code/read-delim.R")
prof <- lineprof(read_delim("diamonds.csv", sep = ","))
shine(prof)
```

This opens a web page that shows your source code annotated with information about memory usage:

![line profiling](memory-lineprof.png)

As well as your original source code, there are four columns:

* t, the time (in seconds) spent on that line of code

* a, the amount of memory (in megabytes), allocated by that line of code.

* r, the amount of memory (in megabytes), released by that line of code. While memory allocation is deterministic, memory release is stochastic: it depends on when the GC was run. This means memory release is not totally accurate: all it really tells you is that memory stopped being using somewhere before this line.

* d, the number of vector duplications that occured. A vector duplication occurs when R copies a vector to preserve its copy-on-modify semantics.

You can hover over any of the bars to get the exact numbers.  For this example, looking at the allocations tells us most of the story:

* `scan()` allocates about 2.5 MB of memory, which is very close to the 2.8 MB of space that file takes up on disk. You wouldn't expect the numbers to be exactly equal because R doesn't need to store the commas, and the global string pool will save some memory.

* Converting the columns to types allocates another 0.6 MB of memory. You'd expect this to also free some memory because we've converted string columns into integer and numeric columns (which should occupy less space), but we can't see those releases because we don't know when `gc()` is called.

* Finally, calling `as.data.frame()` on a list allocates about 1.6 megabytes of memory and performs over 600 duplicates. This is because `as.data.frame()` isn't terribly efficient and ends up copying the input multiple times.

There are two downsides to this display. Firstly, `read_delim()` only takes around half a second, and the resolution of the profiler is around 1ms, so we only get about 500 samples. Secondly, since GC is lazy, we can never tell exactly when memory is released. Both of these things make it harder to understand 

This makes it harder to figure exactly what's going on.  One way to work around this is to use `torture = TRUE`: this forces R to run GC after every allocation. This helps with both problems because memory is freed as soon as possible, and R runs much more slowly (10-100x in my experience), so the resolution of the timer effectively becomes much greater. So only run this once you've isolated a small part of your code that you want to understand the memory usage of, or if you're very patient. In my experience, it helps largely with smaller allocations and associating allocations with exactly the right line of code. It also helps you see when objects would be reclaimed if absolutely necessary.

```{r, eval = FALSE}
prof <- lineprof(read_delim("diamonds.csv", sep = ","), torture = TRUE)
shine(prof)
```

![line profiling with torture](memory-torture.png)

The basic messages remain the same, but we now get a bit more detail:

* We see a big memory release on line 14, because that's the first line after the type conversion, and it represents the memory saved by converting strings to numbers.

Using line profiling takes a little bit of experience, but the tools in this chapter should get you started on your way to understanding what's going on.

## Modification in place

What happens in the following code?

```{r}
x <- 1:10
x[5] <- 10
x
```

There's two possibilities:

1. R modifies the existing `x` in place

2. R makes a copy of `x` in a new location, modifies that new vector, and then then changes the name `x` to point to the new location.

It turns out that R can do either depending on the circumstances. In the example above, it will modify in place, but if another variable also points to x, then it will copy it to a new location:

```{r}
x <- 1:10
y <- x
x[5] <- 10
y[5]
```

To explore what's going on in more detail we need some new tools provided by the `pryr` package. Given a binding name, `address()` tells us the memory location of the SEXP it's bound to, and `refs()` tells us how many bindings point to that location.

```{r}
library(pryr)
x <- 1:10
c(address(x), refs(x))

y <- x
c(address(y), refs(y))
```

(Note that if you're using Rstudio this `refs()` will always return two: the environment browser makes a reference to every object you create.)

Note that refs is only an estimate and it can only distinguish between 1 and more than 1 references. This means that `refs()` returns 2 in both of the following cases:

```{r}
x <- 1:5
y <- x
rm(y)
# Is actually one, because we've deleted y
refs(x)

x <- 1:5
y <- x
z <- x
# Is actually three
refs(x)
```

When `refs(x)` is one, modification will occur in place; when `refs(x)` is two, it will make a copy (so that the other pointers to the object contined unchanged).  Note that in the following example, `y` keeps pointing to the same location while `x` changes.

```{r}
x <- 1:10
y <- x
c(address(x), address(y))

x[5] <- 6L
c(address(x), address(y))
```

Non-primitive functions that touch the object always increment the ref count. Primitive functions are usually written in such a way that they don't increment the ref count. (The reasons are a little complicated, but see the R-devel thread [confused about NAMED](http://r.789695.n4.nabble.com/Confused-about-NAMED-td4103326.html))

```{r}
x <- 1:10
refs(x)
mean(x)
refs(x)

# Touching the object forces an increment
f <- function(x) x
x <- 1:10
refs(x); f(x); refs(x)

# Sum is primitive
x <- 1:10
refs(x); sum(x); refs(x)

# These never evaluated x so don't increment refs
f <- function(x) 10
x <- 1:10; refs(x); f(x); refs(x)

g <- function(x) substitute(x)
x <- 1:10; refs(x); g(x); refs(x)
```

The rules are sufficiently complicated that there's not a lot of point in trying to memorise them; instead approach the problem practically; inserting `refs()` calls into your code to verify whether or not an operation increments the `refs()`.

Generally, any primitive replacement function will modify in place, provided that the object is not referred to elsewhere. This includes `[[<-`, `[<-`, `@<-`, `$<-`, `attr<-`, `attributes<-`, `class<-`, `dim<-`, `dimnames<-`, `names<-`, and `levels<-`. To be precise, all non-primitive functions increment refs, but a primitive function may be written in such a way that it doesn't increment refs.

### Loops

For loops in R have a reputation for being slow, but often this slowness is because instead of modifying in place, you're modifying a copy. Take the following code that subtracts the median from each column of a large data.frame:

```{r, cache = TRUE}
x <- data.frame(matrix(runif(100 * 1e4), ncol = 100))
medians <- vapply(x, median, numeric(1))

system.time({
  for(i in seq_along(medians)) {
    x[, i] <- x[, i] - medians[i]
  }
})
```

It's rather slow - we only have 100 columns and 10,000 rows, but it's still taking over second. We can use `address()` and `refs()` to see what's going on for a small sample of the loop:

```{r, results = 'hide'}
for(i in 1:5) {
  x[, i] <- x[, i] - medians[i]
  print(c(address(x), refs(x)))
}
```

In each iteration `x` is moved to a new location (copying the complete data frame) and `refs(x)` is always 2. This is because `[<-.data.frame` is not a primitive function, so it always increments the refs. We can make the function substantially more efficient by using either a list or matrix instead of a data frame. Modifying a lists and matrices uses `[[<-`, a primitive function, so does not increment refs. Therefore all modifications are in place, and the function is much much faster.

```{r}
y <- as.list(x)
system.time({
  for(i in seq_along(medians)) {
    y[[i]] <- y[[i]] - medians[i]
  }
})

z <- as.matrix(x)
system.time({
  for(i in seq_along(medians)) {
    z[[i]] <- z[[i]] - medians[i]
  }
})
```

### Exercises

* Saying that the code below didn't make any duplications is actually a slight simplification - it makes one duplication. Where does it occur and why? (Hint: look at `refs(y)`)

    ```{r}
    y <- as.list(x)
    for(i in seq_along(medians)) {
      y[[i]] <- y[[i]] - medians[i]
    }
    ```
